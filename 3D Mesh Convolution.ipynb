{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YemmyMMU/Computer-Vision-and-Graphics/blob/master/3D%20Mesh%20Convolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_ipYOe8ljoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_graphics\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_graphics.nn.layer import graph_convolution as graph_conv\n",
        "from tensorflow_graphics.notebooks import mesh_segmentation_dataio as dataio\n",
        "from tensorflow_graphics.notebooks import mesh_viewer\n",
        "\n",
        "path_to_model_zip = tf.keras.utils.get_file(\n",
        "    'model.zip',\n",
        "    origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/model.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_data_zip = tf.keras.utils.get_file(\n",
        "    'data.zip',\n",
        "    origin='https://storage.googleapis.com/tensorflow-graphics/notebooks/mesh_segmentation/data.zip',\n",
        "    extract=True)\n",
        "\n",
        "local_model_dir = os.path.join(os.path.dirname(path_to_model_zip), 'model')\n",
        "test_data_files = [\n",
        "    os.path.join(\n",
        "        os.path.dirname(path_to_data_zip),\n",
        "        'data/Dancer_test_sequence.tfrecords')\n",
        "]\n",
        "    \n",
        "test_io_params = {\n",
        "    'is_training': False,\n",
        "    'sloppy': False,\n",
        "    'shuffle': True,\n",
        "}\n",
        "test_tfrecords = test_data_files\n",
        "\n",
        "\n",
        "input_graph = tf.Graph()\n",
        "with input_graph.as_default():\n",
        "  mesh_load_op = dataio.create_input_from_dataset(\n",
        "      dataio.create_dataset_from_tfrecords, test_tfrecords, test_io_params)\n",
        "  with tf.Session() as sess:\n",
        "    test_mesh_data, test_labels = sess.run(mesh_load_op)\n",
        "\n",
        "input_mesh_data = {\n",
        "    'vertices': test_mesh_data['vertices'][0, ...],\n",
        "    'faces': test_mesh_data['triangles'][0, ...],\n",
        "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[test_labels[0, ...]],\n",
        "}\n",
        "input_viewer = mesh_viewer.Viewer(input_mesh_data)\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    'num_filters': 8,\n",
        "    'num_classes': 16,\n",
        "    'encoder_filter_dims': [32, 64, 128],\n",
        "}\n",
        "\n",
        "\n",
        "def mesh_encoder(batch_mesh_data, num_filters, output_dim, conv_layer_dims):\n",
        "  \"\"\"A mesh encoder using feature steered graph convolutions.\n",
        "\n",
        "    The shorthands used below are\n",
        "      `B`: Batch size.\n",
        "      `V`: The maximum number of vertices over all meshes in the batch.\n",
        "      `D`: The number of dimensions of input vertex features, D=3 if vertex\n",
        "        positions are used as features.\n",
        "\n",
        "  Args:\n",
        "    batch_mesh_data: A mesh_data dict with following keys\n",
        "      'vertices': A [B, V, D] `float32` tensor of vertex features, possibly\n",
        "        0-padded.\n",
        "      'neighbors': A [B, V, V] `float32` sparse tensor of edge weights.\n",
        "      'num_vertices': A [B] `int32` tensor of number of vertices per mesh.\n",
        "    num_filters: The number of weight matrices to be used in feature steered\n",
        "      graph conv.\n",
        "    output_dim: A dimension of output per vertex features.\n",
        "    conv_layer_dims: A list of dimensions used in graph convolution layers.\n",
        "\n",
        "  Returns:\n",
        "    vertex_features: A [B, V, output_dim] `float32` tensor of per vertex\n",
        "      features.\n",
        "  \"\"\"\n",
        "  batch_vertices = batch_mesh_data['vertices']\n",
        "\n",
        "  # Linear: N x D --> N x 16.\n",
        "  vertex_features = tf.keras.layers.Conv1D(16, 1, name='lin16')(batch_vertices)\n",
        "\n",
        "  # graph convolution layers\n",
        "  for dim in conv_layer_dims:\n",
        "    with tf.variable_scope('conv_%d' % dim):\n",
        "      vertex_features = graph_conv.feature_steered_convolution_layer(\n",
        "          vertex_features,\n",
        "          batch_mesh_data['neighbors'],\n",
        "          batch_mesh_data['num_vertices'],\n",
        "          num_weight_matrices=num_filters,\n",
        "          num_output_channels=dim)\n",
        "    vertex_features = tf.nn.relu(vertex_features)\n",
        "\n",
        "  # Linear: N x 128 --> N x 256.\n",
        "  vertex_features = tf.keras.layers.Conv1D(\n",
        "      256, 1, name='lin256')(\n",
        "          vertex_features)\n",
        "  vertex_features = tf.nn.relu(vertex_features)\n",
        "\n",
        "  # Linear: N x 256 --> N x output_dim.\n",
        "  vertex_features = tf.keras.layers.Conv1D(\n",
        "      output_dim, 1, name='lin_output')(\n",
        "          vertex_features)\n",
        "\n",
        "  return vertex_features\n",
        "    \n",
        "def model_fn(features, labels, mode, params):\n",
        "  \"\"\"Returns a mesh segmentation model_fn for use with tf.Estimator.\"\"\"\n",
        "  logits = mesh_encoder(features, params['num_filters'], params['num_classes'],\n",
        "                        params['encoder_filter_dims'])\n",
        "  predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "  outputs = {\n",
        "      'vertices': features['vertices'],\n",
        "      'triangles': features['triangles'],\n",
        "      'num_vertices': features['num_vertices'],\n",
        "      'num_triangles': features['num_triangles'],\n",
        "      'predictions': predictions,\n",
        "  }\n",
        "  # For predictions, return the outputs.\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    outputs['labels'] = features['labels']\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, predictions=outputs)\n",
        "  # Loss\n",
        "  # Weight the losses by masking out padded vertices/labels.\n",
        "  vertex_ragged_sizes = features['num_vertices']\n",
        "  mask = tf.sequence_mask(vertex_ragged_sizes, tf.shape(labels)[-1])\n",
        "  loss_weights = tf.cast(mask, dtype=tf.float32)\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(\n",
        "      logits=logits, labels=labels, weights=loss_weights)\n",
        "  # For training, build the optimizer.\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.AdamOptimizer(\n",
        "        learning_rate=params['learning_rate'],\n",
        "        beta1=params['beta'],\n",
        "        epsilon=params['adam_epsilon'])\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "      train_op = optimizer.minimize(\n",
        "          loss=loss, global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  # For eval, return eval metrics.\n",
        "  eval_ops = {\n",
        "      'mean_loss':\n",
        "          tf.metrics.mean(loss),\n",
        "      'accuracy':\n",
        "          tf.metrics.accuracy(\n",
        "              labels=labels, predictions=predictions, weights=loss_weights)\n",
        "  }\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode, loss=loss, eval_metric_ops=eval_ops)\n",
        "    \n",
        "test_io_params = {\n",
        "    'is_training': False,\n",
        "    'sloppy': False,\n",
        "    'shuffle': True,\n",
        "    'repeat': False\n",
        "}\n",
        "test_tfrecords = test_data_files\n",
        "\n",
        "def predict_fn():\n",
        "  return dataio.create_input_from_dataset(dataio.create_dataset_from_tfrecords,\n",
        "                                          test_tfrecords,\n",
        "                                          test_io_params)\n",
        "\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
        "                                   model_dir=local_model_dir,\n",
        "                                   params=MODEL_PARAMS)\n",
        "test_predictions = estimator.predict(input_fn=predict_fn)\n",
        "\n",
        "    \n",
        "prediction = next(test_predictions)\n",
        "input_mesh_data = {\n",
        "    'vertices': prediction['vertices'],\n",
        "    'faces': prediction['triangles'],\n",
        "}\n",
        "predicted_mesh_data = {\n",
        "    'vertices': prediction['vertices'],\n",
        "    'faces': prediction['triangles'],\n",
        "    'vertex_colors': mesh_viewer.SEGMENTATION_COLORMAP[prediction['predictions']],\n",
        "}\n",
        "\n",
        "input_viewer = mesh_viewer.Viewer(input_mesh_data)\n",
        "prediction_viewer = mesh_viewer.Viewer(predicted_mesh_data)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}