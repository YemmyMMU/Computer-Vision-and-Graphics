{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rotation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YemmyMMU/Computer-Vision-and-Graphics/blob/master/rotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yedj52UblvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_graphics\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow_graphics.geometry.transformation import quaternion\n",
        "from tensorflow_graphics.math import vector\n",
        "from tensorflow_graphics.notebooks import threejs_visualization\n",
        "from tensorflow_graphics.notebooks.resources import tfg_simplified_logo\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# Loads the Tensorflow Graphics simplified logo.\n",
        "vertices = tfg_simplified_logo.mesh['vertices'].astype(np.float32)\n",
        "faces = tfg_simplified_logo.mesh['faces']\n",
        "num_vertices = vertices.shape[0]\n",
        "\n",
        "# Constructs the model.\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Flatten(input_shape=(num_vertices, 3)))\n",
        "model.add(layers.Dense(64, activation=tf.nn.tanh))\n",
        "model.add(layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(layers.Dense(7))\n",
        "\n",
        "\n",
        "def pose_estimation_loss(y_true, y_pred):\n",
        "  \"\"\"Pose estimation loss used for training.\n",
        "\n",
        "  This loss measures the average of squared distance between some vertices\n",
        "  of the mesh in 'rest pose' and the transformed mesh to which the predicted\n",
        "  inverse pose is applied. Comparing this loss with a regular L2 loss on the\n",
        "  quaternion and translation values is left as exercise to the interested\n",
        "  reader.\n",
        "\n",
        "  Args:\n",
        "    y_true: The ground-truth value.\n",
        "    y_pred: The prediction we want to evaluate the loss for.\n",
        "\n",
        "  Returns:\n",
        "    A scalar value containing the loss described in the description above.\n",
        "  \"\"\"\n",
        "  # y_true.shape : (batch, 7)\n",
        "  y_true_q, y_true_t = tf.split(y_true, (4, 3), axis=-1)\n",
        "  # y_pred.shape : (batch, 7)\n",
        "  y_pred_q, y_pred_t = tf.split(y_pred, (4, 3), axis=-1)\n",
        "\n",
        "  # vertices.shape: (num_vertices, 3)\n",
        "  # corners.shape:(num_vertices, 1, 3)\n",
        "  corners = tf.expand_dims(vertices, axis=1)\n",
        "\n",
        "  # transformed_corners.shape: (num_vertices, batch, 3)\n",
        "  # q and t shapes get pre-pre-padded with 1's following standard broadcast rules.\n",
        "  transformed_corners = quaternion.rotate(corners, y_pred_q) + y_pred_t\n",
        "\n",
        "  # recovered_corners.shape: (num_vertices, batch, 3)\n",
        "  recovered_corners = quaternion.rotate(transformed_corners - y_true_t,\n",
        "                                        quaternion.inverse(y_true_q))\n",
        "\n",
        "  # vertex_error.shape: (num_vertices, batch)\n",
        "  vertex_error = tf.reduce_sum((recovered_corners - corners)**2, axis=-1)\n",
        "\n",
        "  return tf.reduce_mean(vertex_error)\n",
        "\n",
        "\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model.compile(loss=pose_estimation_loss, optimizer=optimizer)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "def generate_training_data(num_samples):\n",
        "  # random_angles.shape: (num_samples, 3)\n",
        "  random_angles = np.random.uniform(-np.pi, np.pi,\n",
        "                                    (num_samples, 3)).astype(np.float32)\n",
        "\n",
        "  # random_quaternion.shape: (num_samples, 4)\n",
        "  random_quaternion = quaternion.from_euler(random_angles)\n",
        "\n",
        "  # random_translation.shape: (num_samples, 3)\n",
        "  random_translation = np.random.uniform(-2.0, 2.0,\n",
        "                                         (num_samples, 3)).astype(np.float32)\n",
        "\n",
        "  # data.shape : (num_samples, num_vertices, 3)\n",
        "  data = quaternion.rotate(vertices[tf.newaxis, :, :],\n",
        "                           random_quaternion[:, tf.newaxis, :]\n",
        "                          ) + random_translation[:, tf.newaxis, :]\n",
        "\n",
        "  # target.shape : (num_samples, 4+3)\n",
        "  target = tf.concat((random_quaternion, random_translation), axis=-1)\n",
        "\n",
        "  return np.array(data), np.array(target)\n",
        "\n",
        "num_samples = 10000\n",
        "\n",
        "data, target = generate_training_data(num_samples)\n",
        "\n",
        "print(data.shape)   # (num_samples, num_vertices, 3): the vertices\n",
        "print(target.shape)  # (num_samples, 4+3): the quaternion and translation\n",
        "\n",
        "\n",
        "# Callback allowing to display the progression of the training task.\n",
        "class ProgressTracker(keras.callbacks.Callback):\n",
        "\n",
        "  def __init__(self, num_epochs, step=5):\n",
        "    self.num_epochs = num_epochs\n",
        "    self.current_epoch = 0.\n",
        "    self.step = step\n",
        "    self.last_percentage_report = 0\n",
        "\n",
        "  def on_epoch_end(self, batch, logs={}):\n",
        "    self.current_epoch += 1.\n",
        "    training_percentage = int(self.current_epoch * 100.0 / self.num_epochs)\n",
        "    if training_percentage - self.last_percentage_report >= self.step:\n",
        "      print('Training ' + str(\n",
        "          training_percentage) + '% complete. Training loss: ' + str(\n",
        "              logs.get('loss')) + ' | Validation loss: ' + str(\n",
        "                  logs.get('val_loss')))\n",
        "      self.last_percentage_report = training_percentage\n",
        "      \n",
        "      \n",
        "reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=10,\n",
        "    verbose=0,\n",
        "    mode='auto',\n",
        "    min_delta=0.0001,\n",
        "    cooldown=0,\n",
        "    min_lr=0)\n",
        "    \n",
        "    \n",
        "# Everything is now in place to train.\n",
        "EPOCHS = 100\n",
        "pt = ProgressTracker(EPOCHS)\n",
        "history = model.fit(\n",
        "    data,\n",
        "    target,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2,\n",
        "    verbose=0,\n",
        "    batch_size=32,\n",
        "    callbacks=[reduce_lr_callback, pt])\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(['loss', 'val loss'], loc='upper left')\n",
        "plt.xlabel('Train epoch')\n",
        "_ = plt.ylabel('Error [mean square distance]')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Defines the loss function to be optimized.\n",
        "def transform_points(target_points, quaternion_variable, translation_variable):\n",
        "  return quaternion.rotate(target_points,\n",
        "                           quaternion_variable) + translation_variable\n",
        "\n",
        "class Viewer(object):\n",
        "\n",
        "  def __init__(self, my_vertices):\n",
        "    my_vertices = np.asarray(my_vertices)\n",
        "    context = threejs_visualization.build_context()\n",
        "    light1 = context.THREE.PointLight.new_object(0x808080)\n",
        "    light1.position.set(10., 10., 10.)\n",
        "    light2 = context.THREE.AmbientLight.new_object(0x808080)\n",
        "    lights = (light1, light2)\n",
        "\n",
        "    material = context.THREE.MeshLambertMaterial.new_object({\n",
        "        'color': 0xfffacd,\n",
        "    })\n",
        "\n",
        "    material_deformed = context.THREE.MeshLambertMaterial.new_object({\n",
        "        'color': 0xf0fff0,\n",
        "    })\n",
        "\n",
        "    camera = threejs_visualization.build_perspective_camera(\n",
        "        field_of_view=30, position=(10.0, 10.0, 10.0))\n",
        "\n",
        "    mesh = {'vertices': vertices, 'faces': faces, 'material': material}\n",
        "    transformed_mesh = {\n",
        "        'vertices': my_vertices,\n",
        "        'faces': faces,\n",
        "        'material': material_deformed\n",
        "    }\n",
        "    geometries = threejs_visualization.triangular_mesh_renderer(\n",
        "        [mesh, transformed_mesh],\n",
        "        lights=lights,\n",
        "        camera=camera,\n",
        "        width=400,\n",
        "        height=400)\n",
        "\n",
        "    self.geometries = geometries\n",
        "\n",
        "  def update(self, transformed_points):\n",
        "    self.geometries[1].getAttribute('position').copyArray(\n",
        "        transformed_points.numpy().ravel().tolist())\n",
        "    self.geometries[1].getAttribute('position').needsUpdate = True\n",
        "\n",
        "    \n",
        "def get_random_transform():\n",
        "  # Forms a random translation\n",
        "  with tf.name_scope('translation_variable'):\n",
        "    random_translation = tf.Variable(\n",
        "        np.random.uniform(-2.0, 2.0, (3,)), dtype=tf.float32)\n",
        "\n",
        "  # Forms a random quaternion\n",
        "  hi = np.pi\n",
        "  lo = -hi\n",
        "  random_angles = np.random.uniform(lo, hi, (3,)).astype(np.float32)\n",
        "  with tf.name_scope('rotation_variable'):\n",
        "    random_quaternion = tf.Variable(quaternion.from_euler(random_angles))\n",
        "\n",
        "  return random_quaternion, random_translation\n",
        "\n",
        "\n",
        "random_quaternion, random_translation = get_random_transform()\n",
        "\n",
        "initial_orientation = transform_points(vertices, random_quaternion,\n",
        "                                       random_translation).numpy()\n",
        "viewer = Viewer(initial_orientation)\n",
        "\n",
        "predicted_transformation = model.predict(initial_orientation[tf.newaxis, :, :])\n",
        "\n",
        "predicted_inverse_q = quaternion.inverse(predicted_transformation[0, 0:4])\n",
        "predicted_inverse_t = -predicted_transformation[0, 4:]\n",
        "\n",
        "predicted_aligned = quaternion.rotate(initial_orientation + predicted_inverse_t,\n",
        "                                      predicted_inverse_q)\n",
        "\n",
        "viewer = Viewer(predicted_aligned)\n",
        "    \n",
        "    \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}